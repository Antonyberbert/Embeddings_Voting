---
title: "H1_energyact emb"
output:
  pdf_document: default
  html_document: default
date: "2023-12-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
#Install and load necessary packages
packages <- c("devtools","here","dplyr","tidyverse","readxl","writexl","hunspell","text2vec","car","broom")
for (package in packages) {
  if (!requireNamespace(package, quietly = TRUE)) {
    install.packages(package)
  }
}
lapply(packages,library, character.only=T)
```

```{r}
#Install and load embedR
if (!requireNamespace("embedR", quietly = TRUE)) {
    # If not installed, install it using devtools
    devtools::install_github("dwulff/embedR")
}

library("embedR")
```

```{r}
#read in final dataset
energyact_fin <- read_xlsx(here::here("data","energyact_final.xlsx"))

#load embeddings as R object
embedding <- readRDS(here::here("data","embedding.rds"))
```

```{r,eval=F, echo=F}
#embed chatGPT list
#economy words
#embedR::er_set_tokens("cohere" = "x","huggingface"="x","openai" = "x")

#embedding_econ = er_embed(c("kosten","teuer","preis","arbeit","investition","wirtschaft","finanzierung","budget","ausgaben","steuern","subvention","gewinn","verlust","rendite","sparen","wettbewerb","verschuldung","einkommen","umsatz","markt"),api="cohere",model="embed-multilingual-v3.0")

#environmental prot words
#embedding_env = er_embed(c("umweltschutz","klimaschutz","dringend","notwendig","nachhaltigkeit","erneuerbar","ökologie","biodiversität","artenschutz","ressourcenschonung","emission","klimawandel","ökosystem","umweltbewusstsein","grüne energie","verschmutzung","zeitdruck","landschaftsschutz","naturschutz","tierschutz"),api="cohere",model="embed-multilingual-v3.0")
```

```{r,eval=F, echo=F}
#save embeddings as R object
#saveRDS(embedding_econ, here::here("data","embedding_econ.rds"))

#saveRDS(embedding_env, here::here("data","embedding_env.rds"))

```

```{r}
calculate_max_similarity <- function(embedding_study, embedding_list) {
  # Calculate cosine similarity
  similarity_matrix <- sim2(embedding_study, embedding_list, method = "cosine")
  # Get the maximum similarity score for each word
  max_similarity <- apply(similarity_matrix, 1, max)
  return(max_similarity)
}
```

```{r}
#Load embeddings
embedding_econ <- readRDS(here::here("data","embedding_econ.rds"))

embedding_env <- readRDS(here::here("data","embedding_env.rds"))
# Creating a dataframe from the study embedding matrix
H1_df <- as.data.frame(embedding)

# Save the rownames as the first column in H1_df
H1_df$word <- rownames(H1_df)

# Reset the rownames of H1_df
rownames(H1_df) <- NULL



# Calculate max similarities
H1_df$max_similarity_econ <- calculate_max_similarity(embedding, embedding_econ)
H1_df$max_similarity_env <- calculate_max_similarity(embedding, embedding_env)
```

```{r}
#add similarity columns to energyact_fin
# Get the names of the last two columns
last_two_columns <- names(H1_df)[(ncol(H1_df)-1):ncol(H1_df)]

# Add the last two columns from H1_df to energyact_fin
energyact_fin[last_two_columns] <- H1_df[last_two_columns]
```

```{r}
#Visualize similarity distribution to extract threshold, first for economy words
#First create ordered df
energyact_fin_ordered_econ <- energyact_fin[order(-energyact_fin$max_similarity_econ), ]

#Calculate median similarity
median_similarity_econ <- median(energyact_fin_ordered_econ$max_similarity_econ, na.rm = TRUE)

# Create the plot
plot_sim_econ <- ggplot(energyact_fin_ordered_econ, aes(x = reorder(row.names(energyact_fin_ordered_econ), -max_similarity_econ), y = max_similarity_econ)) +
  geom_hline(yintercept = median_similarity_econ, linetype = "dashed", color = "blue") +
  geom_point() +
  #theme(axis.text.x = element_blank()) +  # Hide x-axis text for legibility
  labs(x = "Words", y = "Max Similarity to Economy", title = "Similarity Scores for Economy Category") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +  # Set y-axis breaks
  theme_classic()

#threshold 0.75
```

```{r}
# Create ordered dataframe based on max_similarity_env
energyact_fin_ordered_env <- energyact_fin[order(-energyact_fin$max_similarity_env), ]

# Calculate median similarity for environmental protection
median_similarity_env <- median(energyact_fin_ordered_env$max_similarity_env, na.rm = TRUE)

# Create the plot for environmental protection similarity
plot_sim_env <- ggplot(energyact_fin_ordered_env, aes(x = reorder(row.names(energyact_fin_ordered_env), -max_similarity_env), y = max_similarity_env)) +
  geom_hline(yintercept = median_similarity_env, linetype = "dashed", color = "blue") +
  geom_point() +
  # theme(axis.text.x = element_blank()) +  # Optionally hide x-axis text for legibility
  labs(x = "Words", y = "Max Similarity to Environmental Protection", title = "Similarity Scores for Environmental Protection Category") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05)) +  # Set y-axis breaks
  theme_classic()

#threshold ~0.82
```

```{r}
#Plot both similarity scores onto one graph facetted by intendedVote along with medians
energyact_fin_long <- energyact_fin %>%
  pivot_longer(cols = c(max_similarity_econ, max_similarity_env),
               names_to = "category",
               values_to = "similarity")

# Median for whole dataset
median_econ_all <- median(energyact_fin$max_similarity_econ, na.rm = TRUE)
median_env_all <- median(energyact_fin$max_similarity_env, na.rm = TRUE)

# Median for intendedVote = 0
median_econ_0 <- median(energyact_fin$max_similarity_econ[energyact_fin$intendedVote == 0], na.rm = TRUE)
median_env_0 <- median(energyact_fin$max_similarity_env[energyact_fin$intendedVote == 0], na.rm = TRUE)

# Median for intendedVote = 1
median_econ_1 <- median(energyact_fin$max_similarity_econ[energyact_fin$intendedVote == 1], na.rm = TRUE)
median_env_1 <- median(energyact_fin$max_similarity_env[energyact_fin$intendedVote == 1], na.rm = TRUE)

#create median df for label in plot
medians_df <- data.frame(
  category = c("max_similarity_econ", "max_similarity_econ", "max_similarity_econ", "max_similarity_env", "max_similarity_env", "max_similarity_env"),
  intendedVote = c("All", "0", "1", "All", "0", "1"),
  median = c(median_econ_all, median_econ_0, median_econ_1, median_env_all, median_env_0, median_env_1),
  label_y = c(median_econ_all, median_econ_0, median_econ_1, median_env_all, median_env_0, median_env_1) + 0.02  # Adjust label position
)


plot_H1_median <- ggplot(energyact_fin_long, aes(x = reorder(word, -similarity), y = similarity, color = category)) +
  geom_point() +
  geom_hline(yintercept = median_econ_all, color = "blue", linetype = "dashed") +
  geom_hline(yintercept = median_env_all, color = "green", linetype = "dashed") +
  geom_hline(yintercept = median_econ_0, data = subset(energyact_fin_long, intendedVote == 0), color = "blue", linetype = "dotted") +
  geom_hline(yintercept = median_econ_1, data = subset(energyact_fin_long, intendedVote == 1), color = "blue", linetype = "dotdash") +
  geom_hline(yintercept = median_env_0, data = subset(energyact_fin_long, intendedVote == 0), color = "green", linetype = "dotted") +
  geom_hline(yintercept = median_env_1, data = subset(energyact_fin_long, intendedVote == 1), color = "green", linetype = "dotdash") +
  annotate("text", x = Inf, y = 1, hjust = 1.1, vjust = 1, 
           label = paste("Median Econ All:", round(median_econ_all, 2), "\n",
                         "Median Env All:", round(median_env_all, 2), "\n",
                         "Median Econ 0:", round(median_econ_0, 2), "\n",
                         "Median Econ 1:", round(median_econ_1, 2), "\n",
                         "Median Env 0:", round(median_env_0, 2), "\n",
                         "Median Env 1:", round(median_env_1, 2)), 
           color = "black", size = 3) +
  scale_color_manual(values = c("max_similarity_econ" = "blue", "max_similarity_env" = "green")) +
  facet_wrap(~ intendedVote) +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  labs(x = "Words", y = "Max Similarity", title = "Similarity Scores for Economy and Environmental Protection Categories") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05))  # Set y-axis breaks
```

```{r}
#Plot both similarity scores onto one graph facetted by intendedVote along with means
# Mean for the whole dataset
mean_econ_all <- mean(energyact_fin$max_similarity_econ, na.rm = TRUE)
mean_env_all <- mean(energyact_fin$max_similarity_env, na.rm = TRUE)

# Mean for intendedVote = 0
mean_econ_0 <- mean(energyact_fin$max_similarity_econ[energyact_fin$intendedVote == 0], na.rm = TRUE)
mean_env_0 <- mean(energyact_fin$max_similarity_env[energyact_fin$intendedVote == 0], na.rm = TRUE)

# Mean for intendedVote = 1
mean_econ_1 <- mean(energyact_fin$max_similarity_econ[energyact_fin$intendedVote == 1], na.rm = TRUE)
mean_env_1 <- mean(energyact_fin$max_similarity_env[energyact_fin$intendedVote == 1], na.rm = TRUE)

plot_H1_mean <- ggplot(energyact_fin_long, aes(x = reorder(word, -similarity), y = similarity, color = category)) +
  geom_point() +
  geom_hline(yintercept = mean_econ_all, color = "blue", linetype = "dashed") +
  geom_hline(yintercept = mean_env_all, color = "green", linetype = "dashed") +
  geom_hline(yintercept = mean_econ_0, data = subset(energyact_fin_long, intendedVote == 0), color = "blue", linetype = "dotted") +
  geom_hline(yintercept = mean_econ_1, data = subset(energyact_fin_long, intendedVote == 1), color = "blue", linetype = "dotdash") +
  geom_hline(yintercept = mean_env_0, data = subset(energyact_fin_long, intendedVote == 0), color = "green", linetype = "dotted") +
  geom_hline(yintercept = mean_env_1, data = subset(energyact_fin_long, intendedVote == 1), color = "green", linetype = "dotdash") +
  annotate("text", x = Inf, y = 1, hjust = 1.1, vjust = 1, 
           label = paste("Mean Econ All:", round(mean_econ_all, 2), "\n",
                         "Mean Env All:", round(mean_env_all, 2), "\n",
                         "Mean Econ 0:", round(mean_econ_0, 2), "\n",
                         "Mean Econ 1:", round(mean_econ_1, 2), "\n",
                         "Mean Env 0:", round(mean_env_0, 2), "\n",
                         "Mean Env 1:", round(mean_env_1, 2)), 
           color = "black", size = 3) +
  scale_color_manual(values = c("max_similarity_econ" = "blue", "max_similarity_env" = "green")) +
  facet_wrap(~ intendedVote) +
  theme_minimal() +
  theme(axis.text.x = element_blank()) +
  labs(x = "Words", y = "Max Similarity", title = "Similarity Scores for Economy and Environmental Protection Categories") +
  scale_y_continuous(breaks = seq(0, 1, by = 0.05))

```
```{r}
#Export plots
# Export plot_sim_econ
ggsave("plot_sim_econ.png", plot = plot_sim_econ, width = 10, height = 8, dpi = 300)

# Export plot_sim_env
ggsave("plot_sim_env.png", plot = plot_sim_env, width = 10, height = 8, dpi = 300)

# Export plot_H1_median
ggsave("plot_H1_median.png", plot = plot_H1_median, width = 10, height = 8, dpi = 300)

# Export plot_H1_mean
ggsave("plot_H1_mean.png", plot = plot_H1_mean, width = 10, height = 8, dpi = 300)

```


```{r}
#How many words are ambivalent (above both thresholds)
both_conditions_count <- sum(energyact_fin$max_similarity_econ > 0.75 & energyact_fin$max_similarity_env > 0.82)
print(both_conditions_count)
```

```{r}
#Category assignment
energyact_fin$category <- ifelse(energyact_fin$max_similarity_econ > 0.75 & energyact_fin$max_similarity_env > 0.82, "amb",
                                 ifelse(energyact_fin$max_similarity_econ > 0.75, "econ",
                                        ifelse(energyact_fin$max_similarity_env > 0.82, "env", "other")))
```

```{r}
#Manually categorize 50 rows to check validity of embedding categorization
set.seed(42)  # Setting a seed for reproducibility
sampled_rows <- sample_n(energyact_fin, 50)

# View the sampled rows
print(sampled_rows$word)

sampled_rows$manual_cat <- c("other","other","other","econ","other","other","other","other","other","other","other","other","other","env","other","other","env","env","other","econ","other","other","other","other","econ","other","other","econ","other","econ","env","other","other","other","other","other","other","other","env","econ","other","other","other","other","other","other","other","other","other","env")
```

```{r}
# Compare manual labeling with automated labeling
correct_matches <- sum(sampled_rows$category == sampled_rows$manual_cat)

# Calculate the ratio
match_ratio <- correct_matches / nrow(sampled_rows)

# Print the ratio
print(match_ratio)
```

```{r}
# Subset to find rows where categorizations do not match
non_matching_rows <- sampled_rows[sampled_rows$category != sampled_rows$manual_cat, ]

# Print the non-matching rows
print(non_matching_rows[, c("word", "category", "manual_cat")])

```

```{r}
#Frequency table of categories mentioned by participants depending on vote, and for t1 & t2 separately
category_counts <- energyact_fin %>%
  group_by(participantID, intendedVote, category) %>%
  summarize(count = n(), .groups = 'drop') %>%
  ungroup()

summary_table <- category_counts %>%
  group_by(intendedVote) %>%
  summarize(econ_count = sum(count[category == "econ"]),
            env_count = sum(count[category == "env"]),
            amb_count = sum(count[category == "amb"]),
            other_count = sum(count[category == "other"]), 
            .groups = 'drop')

print(summary_table)

# For wave t1
category_counts_t1 <- energyact_fin %>%
  filter(wave == "t1") %>%
  group_by(participantID, intendedVote, category) %>%
  summarize(count = n(), .groups = 'drop') %>%
  ungroup()

summary_table_t1 <- category_counts_t1 %>%
  group_by(intendedVote) %>%
  summarize(econ_count = sum(count[category == "econ"]),
            env_count = sum(count[category == "env"]),
            amb_count = sum(count[category == "amb"]),
            other_count = sum(count[category == "other"]), 
            .groups = 'drop')

print(summary_table_t1)

# For wave t2
category_counts_t2 <- energyact_fin %>%
  filter(wave == "t2") %>%
  group_by(participantID, intendedVote, category) %>%
  summarize(count = n(), .groups = 'drop') %>%
  ungroup()

summary_table_t2 <- category_counts_t1 %>%
  group_by(intendedVote) %>%
  summarize(econ_count = sum(count[category == "econ"]),
            env_count = sum(count[category == "env"]),
            amb_count = sum(count[category == "amb"]),
            other_count = sum(count[category == "other"]), 
            .groups = 'drop')

print(summary_table_t2)
```
```{r}
#whole dataset
participant_summary <- energyact_fin %>%
  group_by(participantID) %>%
  summarize(total_words = n(),
            econ_words = sum(category == "econ"),
            env_words = sum(category == "env"),
            intendedVote = first(intendedVote)) %>%
  mutate(proportion_econ = econ_words / total_words,
         proportion_env = env_words / total_words) %>%
  ungroup()

# For wave t1
participant_summary_t1 <- energyact_fin %>%
  filter(wave == "t1") %>%
  group_by(participantID) %>%
  summarize(total_words = n(),
            econ_words = sum(category == "econ"),
            env_words = sum(category == "env"),
            intendedVote = first(intendedVote)) %>%
  mutate(proportion_econ = econ_words / total_words,
         proportion_env = env_words / total_words) %>%
  ungroup()


# For wave t2
participant_summary_t2 <- energyact_fin %>%
  filter(wave == "t2") %>%
  group_by(participantID) %>%
  summarize(total_words = n(),
            econ_words = sum(category == "econ"),
            env_words = sum(category == "env"),
            intendedVote = first(intendedVote)) %>%
  mutate(proportion_econ = econ_words / total_words,
         proportion_env = env_words / total_words) %>%
  ungroup()
```

```{r}
# For the whole dataset
summary_stats <- participant_summary %>%
  group_by(intendedVote) %>%
  summarise(
    mean_proportion_econ = mean(proportion_econ, na.rm = TRUE),
    sd_proportion_econ = sd(proportion_econ, na.rm = TRUE),
    mean_proportion_env = mean(proportion_env, na.rm = TRUE),
    sd_proportion_env = sd(proportion_env, na.rm = TRUE)
  )

print(summary_stats)

intended_vote_distribution <- participant_summary %>%
  group_by(intendedVote) %>%
  summarise(
    count = n(),
    proportion = count / n()
  )

print(intended_vote_distribution)

# For wave t1
summary_stats_t1 <- participant_summary_t1 %>%
  group_by(intendedVote) %>%
  summarise(
    mean_proportion_econ = mean(proportion_econ, na.rm = TRUE),
    sd_proportion_econ = sd(proportion_econ, na.rm = TRUE),
    mean_proportion_env = mean(proportion_env, na.rm = TRUE),
    sd_proportion_env = sd(proportion_env, na.rm = TRUE)
  )

print(summary_stats_t1)

intended_vote_distribution_t1 <- participant_summary_t1 %>%
  group_by(intendedVote) %>%
  summarise(
    count = n(),
    proportion = count / n()
  )

print(intended_vote_distribution_t1)

# For wave t2
summary_stats_t2 <- participant_summary_t2 %>%
  group_by(intendedVote) %>%
  summarise(
    mean_proportion_econ = mean(proportion_econ, na.rm = TRUE),
    sd_proportion_econ = sd(proportion_econ, na.rm = TRUE),
    mean_proportion_env = mean(proportion_env, na.rm = TRUE),
    sd_proportion_env = sd(proportion_env, na.rm = TRUE)
  )

print(summary_stats_t2)

intended_vote_distribution_t2 <- participant_summary_t2 %>%
  group_by(intendedVote) %>%
  summarise(
    count = n(),
    proportion = count / n()
  )

print(intended_vote_distribution_t2)

```

```{r}
#Assumptions for logistic regression
table(participant_summary$intendedVote)

#Data independent (single participants)

#Multicollinearity
model_for_vif <- glm(intendedVote ~ proportion_econ + proportion_env, data = participant_summary, family = binomial)
vif_result <- vif(model_for_vif)
print(vif_result)
#1.007 = no significant multicollinearity

#Outliers
cooks_distances <- broom::augment(model_for_vif) %>% dplyr::select(.cooksd)
plot(cooks_distances$.cooksd, ylab = "Cook's Distance")
```


```{r}
# Adding a small constant to probabilities
epsilon <- 0.001
participant_summary$log_odds_intendedVote <- with(participant_summary, 
                                                  log((intendedVote + epsilon) / (1 - intendedVote + epsilon)))

# Plot for proportion_econ
ggplot(participant_summary, aes(x = proportion_econ, y = log_odds_intendedVote)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Log Odds of IntendedVote vs Proportion of Econ Words",
       x = "Proportion of Econ Words", y = "Log Odds of IntendedVote")

# Plot for proportion_env
ggplot(participant_summary, aes(x = proportion_env, y = log_odds_intendedVote)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Log Odds of IntendedVote vs Proportion of Env Words",
       x = "Proportion of Env Words", y = "Log Odds of IntendedVote")

```



```{r}
#Logistic regression whole dataset
model <- glm(intendedVote ~ proportion_econ + proportion_env, family = binomial(link = "logit"), data = participant_summary)
summary(model)

# Extracting model coefficients
coefficients <- coef(model)

# Calculating odds ratios
odds_ratio_econ <- exp(coefficients["proportion_econ"])
odds_ratio_env <- exp(coefficients["proportion_env"])

# Print the odds ratios
print(paste("Odds Ratio for proportion_econ:", odds_ratio_econ))
print(paste("Odds Ratio for proportion_env:", odds_ratio_env))
```

```{r}
# Logistic regression for wave t1
model_t1 <- glm(intendedVote ~ proportion_econ + proportion_env, family = binomial(link = "logit"), data = participant_summary_t1)
summary(model_t1)

# Extracting model coefficients
coefficients_t1 <- coef(model_t1)

# Calculating odds ratios
odds_ratio_econ_t1 <- exp(coefficients_t1["proportion_econ"])
odds_ratio_env_t1 <- exp(coefficients_t1["proportion_env"])

# Print the odds ratios
print(paste("Odds Ratio for proportion_econ:", odds_ratio_econ_t1))
print(paste("Odds Ratio for proportion_env:", odds_ratio_env_t1))
```

```{r}
# Logistic regression for wave t2
model_t2 <- glm(intendedVote ~ proportion_econ + proportion_env, family = binomial(link = "logit"), data = participant_summary_t2)
summary(model_t2)

# Extracting model coefficients
coefficients_t2 <- coef(model_t2)

# Calculating odds ratios
odds_ratio_econ_t2 <- exp(coefficients_t2["proportion_econ"])
odds_ratio_env_t2 <- exp(coefficients_t2["proportion_env"])

# Print the odds ratios
print(paste("Odds Ratio for proportion_econ:", odds_ratio_econ_t2))
print(paste("Odds Ratio for proportion_env:", odds_ratio_env_t2))
```
```{r}
#Bonferroni Correction for multiple testing
# Extract p-values from each model
# Replace 'summary(model)$coefficients' with 'coefficients' if it contains the full summary
p_value_econ_full <- summary(model)$coefficients["proportion_econ", "Pr(>|z|)"]
p_value_env_full <- summary(model)$coefficients["proportion_env", "Pr(>|z|)"]

p_value_econ_t1 <- summary(model_t1)$coefficients["proportion_econ", "Pr(>|z|)"]
p_value_env_t1 <- summary(model_t1)$coefficients["proportion_env", "Pr(>|z|)"]

p_value_econ_t2 <- summary(model_t2)$coefficients["proportion_econ", "Pr(>|z|)"]
p_value_env_t2 <- summary(model_t2)$coefficients["proportion_env", "Pr(>|z|)"]

# Combine all p-values
p_values_all <- c(p_value_econ_full, p_value_env_full, 
                  p_value_econ_t1, p_value_env_t1, 
                  p_value_econ_t2, p_value_env_t2)

# Apply Bonferroni correction
adjusted_p_values <- p.adjust(p_values_all, method = "bonferroni")

# Print adjusted p-values
print(adjusted_p_values)
```

```{r}
# Log-likelihood of the model for entire dataset
log_likelihood_model <- logLik(model)

# Log-likelihood of the null model (only intercept)
null_model <- glm(intendedVote ~ 1, family = binomial(link = "logit"), data = participant_summary)
log_likelihood_null <- logLik(null_model)

cox_snell_r_squared <- 1 - exp((2/nrow(participant_summary)) * (log_likelihood_null - log_likelihood_model))

nagelkerke_r_squared <- cox_snell_r_squared / (1 - exp((2 * log_likelihood_null) / nrow(participant_summary)))

print(paste("Cox and Snell R-squared:", cox_snell_r_squared))
print(paste("Nagelkerke R-squared:", nagelkerke_r_squared))
```

```{r}
# Log-likelihood of the model for wave t1
log_likelihood_model_t1 <- logLik(model_t1)

# Log-likelihood of the null model (only intercept)
null_model_t1 <- glm(intendedVote ~ 1, family = binomial(link = "logit"), data = participant_summary_t1)
log_likelihood_null_t1 <- logLik(null_model_t1)

cox_snell_r_squared_t1 <- 1 - exp((2/nrow(participant_summary_t1)) * (log_likelihood_null_t1 - log_likelihood_model_t1))

nagelkerke_r_squared_t1 <- cox_snell_r_squared_t1 / (1 - exp((2 * log_likelihood_null_t1) / nrow(participant_summary_t1)))

print(paste("Cox and Snell R-squared:", cox_snell_r_squared_t1))
print(paste("Nagelkerke R-squared:", nagelkerke_r_squared_t1))
```

```{r}
# Log-likelihood of the model for wave t2
log_likelihood_model_t2 <- logLik(model_t2)

# Log-likelihood of the null model (only intercept)
null_model_t2 <- glm(intendedVote ~ 1, family = binomial(link = "logit"), data = participant_summary_t2)
log_likelihood_null_t2 <- logLik(null_model_t2)

cox_snell_r_squared_t2 <- 1 - exp((2/nrow(participant_summary_t2)) * (log_likelihood_null_t2 - log_likelihood_model_t2))

nagelkerke_r_squared_t2 <- cox_snell_r_squared_t2 / (1 - exp((2 * log_likelihood_null_t2) / nrow(participant_summary_t2)))

print(paste("Cox and Snell R-squared:", cox_snell_r_squared_t2))
print(paste("Nagelkerke R-squared:", nagelkerke_r_squared_t2))
```





```{r,eval=F,echo=F}
# Binary variables for whether a participant mentioned a category
energyact_fin_binary <- energyact_fin %>%
  group_by(participantID) %>%
  summarize(econ_mentioned = as.numeric(any(category == "econ")),
            env_mentioned = as.numeric(any(category == "env")),
            intendedVote = first(intendedVote),
            .groups = 'drop')

```

```{r,eval=F,echo=F}
# Model for H1a
# Combined model for H1a and H1b
model_H1 <- glm(intendedVote ~ econ_mentioned + env_mentioned, family = binomial(link = "logit"), data = energyact_fin_binary)
summary(model_H1)

# Calculating odds ratios from the model coefficients
odds_ratio_econ <- exp(coef(model_H1)["econ_mentioned"])
odds_ratio_env <- exp(coef(model_H1)["env_mentioned"])

# Print the odds ratios
print("Odds Ratio for econ_mentioned:")
print(odds_ratio_econ)

print("Odds Ratio for env_mentioned:")
print(odds_ratio_env)
```

```{r,eval=F, echo=F}
# Cox and Snell R-squared
null_log_likelihood <- logLik(glm(intendedVote ~ 1, family = binomial(link = "logit"), data = energyact_fin_binary))
model_log_likelihood <- logLik(model_H1)
cox_snell_r_squared <- 1 - exp((2/nrow(energyact_fin_binary)) * (null_log_likelihood - model_log_likelihood))
print(paste("Cox and Snell R-squared:", cox_snell_r_squared))

# Nagelkerke R-squared
nagelkerke_r_squared <- cox_snell_r_squared / (1 - exp((2 * null_log_likelihood) / nrow(energyact_fin_binary)))
print(paste("Nagelkerke R-squared:", nagelkerke_r_squared))
```
